---
title: "finalproject_movies"
author: "Zubair Lalani, Saurav Sharma, David Moreno"
date: "2024-12-11"
output:
  pdf_document: default
  html_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

# Preface for the Professor
Our original proposal was built around a Stress-related dataset which, through exploration, we realized was generated in a uniformly distributed manner. This proved impossible for us to get meaningful insights from it and resulted in us changing direction to use this IMDB dataset. 

# Introduction
In today’s world, films span a wide range of genres, ratings, and certifications, and online platforms like IMDb serve as a hub where viewers rate and discuss these titles. Understanding what features might influence a film’s rating can be valuable for researchers, the film industry, and enthusiasts alike. Are movies with certain content advisories (such as strong violence or profanity) associated with higher or lower viewer ratings? Does the level of audience engagement, as measured by the number of votes cast, correlate with the final rating a film receives? Furthermore, how might genre or other attributes help us model and predict a film’s average IMDb rating?

To explore these questions, we will analyze a dataset derived from IMDb listings. This dataset includes films along with their IMDb ratings, number of votes, runtime, certification rating, and several qualitative content advisories (e.g., for nudity, violence, profanity, alcohol use, and frightening elements). It also includes genre information, which is provided as multiple genres associated with each title. Our goal is to build a regression model that helps explain and predict the IMDb rating of a film based on these characteristics.

We obtained the data from an IMDb dataset that compiles various features of films and television episodes. For this project, we focus only on feature films and exclude television episodes to ensure a more uniform set of observations. After data cleaning and preprocessing, we will examine relationships between IMDb rating and predictors like the number of votes, content advisories, duration, certification rating, and the presence or absence of certain genres. Since IMDb ratings are widely recognized by both casual viewers and industry professionals, a model that can successfully characterize these ratings may provide insights into patterns behind audience reception. The final aim is not to arrive at a single “correct” model, but rather to use the tools from our regression analysis toolkit -- such as transformations, dummy variables, residual diagnostics, and model selection -- to develop a model that fits the data well, checks standard regression assumptions, and ultimately offers meaningful interpretation in the context of film rating prediction.

# Methods
To create a suitable predictive model, we followed a systematic approach that involved data preparation, exploratory analysis, and iterative model fitting. We began by loading the dataset (imdb.csv) and performing initial checks on its structure and contents. 

```{r}
imdb_data = read.csv("imdb.csv", header=TRUE)
str(imdb_data)
```

The original dataset included various observations of films and television episodes, along with attributes such as IMDb rating, number of votes, genre listing, and content advisories. Our first task was to ensure data quality, which include:

## Filtering and Cleaning

We removed rows with missing values to maintain the integrity of our analysis.

```{r}
imdb_data = na.omit(imdb_data)

imdb_data = subset(imdb_data, Rate != "No Rate")
imdb_data = subset(imdb_data, Nudity != "No Rate")
imdb_data = subset(imdb_data, Violence != "No Rate")
imdb_data = subset(imdb_data, Profanity != "No Rate")
imdb_data = subset(imdb_data, Alcohol != "No Rate")
imdb_data = subset(imdb_data, Frightening != "No Rate")
```

We excluded television episodes and focused solely on feature films, removing any variables (such as episode count and type indicators) that were no longer relevant.

```{r}
imdb_data = subset(imdb_data, Type == "Film")
imdb_data = subset(imdb_data, select = -c(Episodes, Type)) # only considering films so episode count and type (series or film) not needed
```

We addressed duplicates in the data, retaining only unique films to avoid skewing results with repeated entries.

```{r}
imdb_data = imdb_data[!duplicated(imdb_data), ]
```
We also converted variables to appropriate data types. For example, the ratings and votes were coerced into numeric form, while certifications and advisories were treated as factors.
```{r}
imdb_data$Rate = as.numeric(imdb_data$Rate)
imdb_data$Votes = as.numeric(gsub(",", "", imdb_data$Votes))
imdb_data$Duration = as.numeric(imdb_data$Duration)
imdb_data$Certificate = as.factor(imdb_data$Certificate)
imdb_data$Nudity = as.factor(imdb_data$Nudity)
imdb_data$Violence = as.factor(imdb_data$Violence)
imdb_data$Profanity = as.factor(imdb_data$Profanity)
imdb_data$Frightening = as.factor(imdb_data$Frightening)
```
## Data Transformation and Feature Engineering 

The Genre variable was provided as a comma-separated string (e.g., "Adventure, Drama, Fantasy"). To include genre information in a regression framework, we transformed this categorical data into multiple binary (dummy) variables, one for each unique genre.

```{r}
genres_list = strsplit(imdb_data$Genre, ", ")
unique_genres = unique(unlist(genres_list))

# also need to sanitize; can't have "Sci-Fi", should instead be "Sci.Fi"
for (genre in unique_genres) {
  imdb_data[[make.names(genre)]] = sapply(genres_list, function(genres) genre %in% genres)
}

# remove original Genre column from dataset
imdb_data = imdb_data[, !names(imdb_data) %in% "Genre"]
```

## Model Construction and Refinement

We started with a multiple linear regression model that included predictors determined through playing around with scatter plots, as well as using our intuition. The predictors include duration, certification rating (PG-13, R, etc), and binary variables for each content advisory category (nudity, violence, profanity, alcohol, frightening elements). We incorporated all genre dummies into the model to capture the effect of various film types (e.g., Action, Drama, Comedy) on ratings.


```{r}
set.seed(1234)
initial_model = lm(Rate ~ Duration + Certificate + Nudity + Violence + Profanity + 
              Alcohol + Frightening + Action + Adventure + Thriller + Crime + Drama + 
              Sci.Fi + Comedy + History + Fantasy + Horror + Mystery + Animation + 
              Family + Romance + Western + Musical + Biography + Music + War + 
              Sport + Short + Film.Noir + Documentary, data = imdb_data)
```

As expected, the results of our initial model were rather poor (see "Results" section for more details). Therefore, after doing some investigation, we decided to include 2 more predictors that we believe would improve the model. These predictors were "Votes" and "Date". We applied a log transformation to "Votes" because it had a large range.

```{r}
set.seed(1234)
genre_terms = paste(make.names(unique_genres), collapse = " + ")
formula = as.formula(paste("Rate ~ log(Votes) + Date + Duration + Certificate + Nudity + Violence + Profanity + Alcohol + Frightening +", genre_terms))
#initial_model = lm(formula, data = imdb_data)
# CANNOT USE A FORMULA CREATED THIS WAY ^. Boxcox library will error out. 

additive_model = lm(Rate ~ log(Votes) + Date + Duration + Certificate + Nudity + Violence + Profanity + 
              Alcohol + Frightening + Action + Adventure + Thriller + Crime + Drama + 
              Sci.Fi + Comedy + History + Fantasy + Horror + Mystery + Animation + 
              Family + Romance + Western + Musical + Biography + Music + War + 
              Sport + Short + Film.Noir + Documentary, data = imdb_data)
```

We defined a diagnostics helper function to easily determine if constraints are being satisfied. 

```{r}
diagnostics = function(model, pcol = "grey", lcol = "dodgerblue", alpha = 0.05, plotit = TRUE, testit = TRUE) {
  output = list()
  if (testit) {
    shapiro_test = shapiro.test(residuals(model))
    p_val = shapiro_test$p.value
    decision = ifelse(p_val < alpha, "Reject", "Fail to Reject")
    output$p_val = p_val
    output$decision = decision
  }
  if (plotit) {
    par(mfrow = c(1, 2))
    plot(model$fitted.values, residuals(model),
         col = pcol, pch = 19,
         xlab = "Fitted", ylab = "Residuals",
         main = "Residuals vs Fitted")
    abline(h = 0, col = lcol, lwd=3)
    qqnorm(residuals(model), col = pcol, pch = 19, main = "Normal Q-Q Plot")
    qqline(residuals(model), col = lcol, lwd=3)
  }
  return(output)
}
```


```{r}
additive_model_summary = summary(additive_model)
diagnostics_result = diagnostics(additive_model)
```

The diagnostics for our additive model were lukewarm. The `Residuals vs Fitted` showed a slight, but discernible, funnel shape. The `Normal Q-Q` plot showed deviations from the diagonal line at tail end of the lower-quantiles. This indicated that the residuals are not perfectly normally distributed. The Shapiro-Wilk test corroborated this finding, with a p-value of `r diagnostics_result$p_val`, leading to the rejection of the null hypothesis of normality. Notably, the Adjusted R-Squared for this additive model was `r summary(additive_model)$adj.r.squared`.

Given these results, we aimed to iteratively refine our model first by removing any influential points which might have impacted the fit of our initial model.

```{r}
set.seed(1234)
leverage = hatvalues(additive_model)
cooks_distance = cooks.distance(additive_model)

influential_points = which(cooks_distance > 4 / nrow(imdb_data))

imdb_data_cleaned = imdb_data[-influential_points, ]
model_without_influential_pts = lm(Rate ~ log(Votes) + Date + Duration + Certificate + Nudity + Violence + Profanity + Alcohol + Frightening + Action + Adventure + Thriller + Crime + Drama + Sci.Fi + Comedy + History + Fantasy + Horror + Mystery + Animation + Family + Romance + Western + Musical + Biography + Music + War + Sport + Short + Film.Noir + Documentary, 
                                   data = imdb_data_cleaned)

model_without_influential_pts_summary = summary(model_without_influential_pts)
```

Removing influential points did not significantly change our diagnostic plots, and the Adjusted R-Squared of this new model was now `r model_without_influential_pts_summary$adj.r.squared`.

Continuing our exploration, we evaluated transformations of the response to improve normality of residuals and better meet linear regression assumptions. Ultimately, we found Box-cox transformation of our response variable to improve our model in these regards.

```{r}
library(MASS)
set.seed(1234)

any(imdb_data_cleaned$Rate <= 0) # no shift needed if non-positive values

boxcox_result = boxcox(model_without_influential_pts, lambda = seq(-2, 2, by = 0.1))
optimal_lambda = boxcox_result$x[which.max(boxcox_result$y)]
imdb_data_cleaned$Rate_transformed = (imdb_data_cleaned$Rate^optimal_lambda - 1) / optimal_lambda

boxcox_transformed_model = lm(Rate_transformed ~ log(Votes) + Date + Duration + Certificate + Nudity + Violence
                       + Profanity + Alcohol + Frightening + Action + Adventure + Thriller + 
                       Crime + Drama + Sci.Fi + Comedy + History + Fantasy + Horror + Mystery + 
                       Animation + Family + Romance + Western + Musical + Biography + Music + 
                       War + Sport + Short + Film.Noir + Documentary, 
                       data = imdb_data_cleaned)

boxcox_transformed_model_summary = summary(boxcox_transformed_model)
```

The Boxcox transformation on the response variable seemed to have a decent impact on the diagnostics of our model. 

We next reviewed variance inflation factors (VIFs) to check for multicollinearity issues and considered interaction terms where conceptually meaningful (e.g., between certain advisory categories and film genres).

```{r}
car::vif(boxcox_transformed_model)
```

From this, we decided that it might be worth dropping a feature which seemed to be contributing to collinearity -- namely `Certificate`.

Additionally, we believed that the relationship between some of our predictor variables would be better modeled as interactions rather than just additively. For example, we believed that the feature of `Frightening` would be better suited when associated with the `Horror` genre. Additionally, we combined `Alcohol`, `Nudity`, `Violence`, and `Profanity` through interaction as we believed that this interaction could account for the `Certificate` feature which we removed due to collinearity issues.

```{r}
set.seed(1234)
interaction_model = lm(Rate_transformed ~ log(Votes) + Date + Duration + (Nudity + Violence + Profanity + Alcohol)^2 + Frightening:Horror + Action + Adventure + Thriller + 
                       Crime + Drama + Sci.Fi + Comedy + Fantasy + Horror + Mystery + 
                       Animation + Family + Romance + Western + Musical + Biography + Music + 
                       War + Sport + Short + Film.Noir + Documentary, 
                       data = imdb_data_cleaned)

interaction_model_summary = summary(interaction_model)
```
With the awareness that our model seemed to be getting larger in features, we decided to take one final approach in our iteration. We decided to use [backward] stepwise selection to see if a reduced model fit equally or better. 

```{r echo = TRUE, results='hide'}
step_reduced_model = step(interaction_model, direction = "backward")
step_reduced_model_summary = summary(step_reduced_model)
```

Throughout this process, we documented each step of the analysis. We emphasize that our approach is not about applying every possible method, but rather about using appropriate methods to arrive at a well-justified final model that addresses a meaningful question: How well can we predict and explain a film’s IMDb rating using its measurable attributes?

# Results

Below you can find an assessment of the models we tried. The order of each section match our approach chronologically. Note that this is a subset of the models we tried as we added/removed predictors along the way as we experimented. This subset gives the best overall picture of our investigation and documents the results of major adjustments to our model.

## Initial additive model

Adjusted R-Square: `r summary(initial_model)$adj.r.squared`

```{r}
initial_model_diagnostics = diagnostics(initial_model)
```


## Additive Model Results

Adjusted R-Square: `r summary(additive_model)$adj.r.squared`

```{r}
additive_model_diagnostics = diagnostics(additive_model)
```

## Model without influential points

Adjusted R-Square: `r summary(model_without_influential_pts)$adj.r.squared`

```{r}
model_without_influential_diagnostics = diagnostics(model_without_influential_pts)
```


## Model with Box-Cox transformation applied

Adjusted R-Square: `r summary(boxcox_transformed_model)$adj.r.squared`

```{r}
box_cox_mdl_diagnostics = diagnostics(boxcox_transformed_model)
```


## Interaction Model (and remove variables to fix collinearity issues)

Adjusted R-Square: `r summary(interaction_model)$adj.r.squared`

```{r}
interaction_model_diagnostics = diagnostics(interaction_model)
```


## Backward stepwise selection model

Adjusted R-Square: `r summary(step_reduced_model)$adj.r.squared`

```{r}
step_reduced_model_diagnostics = diagnostics(step_reduced_model)
```



## Selecting a model

TODO: Not sure which model to pick as our final model because it seems box-cox technical performs best at ~0.57, whereas the removing collinearity and adding interactions terms both perform at ~0.56. I've written the text below assuming that we are picking the stepwise selection model, but not sure if that's the right decision or not.

Based on the results above, we ultimately landed on our model that used backward selection as it had a good adjusted R Squared value, least number of predictors, while also having the best diagnostic plots. Below, we have displayed the formula for our final regression model.

```{r}
summary(step_reduced_model)$call
```

This isn't a perfect model, but we feel reasonably confident that this model provides solid information regarding what factors impact a film's rating. Given that the data lacks some key informative predictors such as information regarding the cast/director of the movie, we believe that this model performs reasonably well compared to other models.

To illustrate this, we compare our prediction ability from our starting model to the final model we chose.


```{R}
set.seed(391993)

# split data
train_index = sample(seq_len(nrow(imdb_data_cleaned)), size = 0.9 * nrow(imdb_data_cleaned))
train_data = imdb_data_cleaned[train_index, ]
test_data = imdb_data_cleaned[-train_index, ]

# initial model trained, make sure to have the data only be pointing to train_data
trained_intial_model = initial_model;
# "final" model trained
trained_final_model = step_reduced_model

# predict for both
predictions_initial = predict(trained_intial_model, newdata=test_data)
predictions_final = predict(trained_final_model, newdata=test_data)

# compare RMSEs
expected = test_data$Rate_transformed
rmse_value_initial_model = sqrt(mean((expected - predictions_initial)^2))
rmse_value_final_model = sqrt(mean((expected - predictions_final)^2))
```

The RMSE of our initial model was `r rmse_value_initial_model`, whereas the RMSE value of our final model is `r rmse_value_final_model` which demonstrates the improvement we achieved over the course of our investigation.

# Discussion
Our project was aimed to explore the influence in IMDb ratings of films using the dataset. various factors in this study like predictors, votes, genre, and content. 
We worked on an iterative approach to develop several regression models to predict IMDB ratings.Our process was data cleaning, defining features, and the different statistical method.
With our iterative trails, we were able to refine our model model to achieve a substantial improvement from our initial model, we got the RMSE to reduce from 17.15 to 4.15. Our process showed how important it was to use transformer, the interaction terms, and model selection for regression analysis. We did not get a "perfect" model, but working to get a better result for analysis was beneficial for us to find what else is helpful. More studies can expand from our work by using other predictors and exploring other frameworks.